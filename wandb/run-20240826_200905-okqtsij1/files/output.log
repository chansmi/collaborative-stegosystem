
Using BitsAndBytesConfig for NVIDIA GPU
config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 614/614 [00:00<00:00, 2.16MB/s]
model.safetensors.index.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26.8k/26.8k [00:00<00:00, 105MB/s]
Downloading shards:   0%|                                                                                                                                                         | 0/2 [00:00<?, ?it/s]



























































Downloading shards:  50%|████████████████████████████████████████████████████████████████████████                                                                        | 1/2 [02:00<02:00, 120.94s/it]


























Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:55<00:00, 87.54s/it]██████████████████████████████████▌| 3.49G/3.50G [00:53<00:00, 95.8MB/s]
Traceback (most recent call last):
  File "/root/collaborative-stegosystem/main.py", line 30, in <module>
    main()
  File "/root/collaborative-stegosystem/main.py", line 24, in main
    trainer = CollaborativePPOTrainer(config)
  File "/root/collaborative-stegosystem/src/ppo_trainer.py", line 22, in __init__
    self.alice = create_agent(config, 'Alice')
  File "/root/collaborative-stegosystem/src/models.py", line 24, in create_agent
    model = AutoModelForCausalLM.from_pretrained(
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 3826, in from_pretrained
    config = cls._autoset_attn_implementation(
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 1556, in _autoset_attn_implementation
    cls._check_and_enable_flash_attn_2(
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 1658, in _check_and_enable_flash_attn_2
    raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")
ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.