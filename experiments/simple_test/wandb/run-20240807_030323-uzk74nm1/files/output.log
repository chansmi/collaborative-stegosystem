2024-08-07 03:03:23,911 - INFO - Script arguments: ScriptArguments(model_name='huggyllama/llama-7b', dataset_name='Anthropic/hh-rlhf', rm_adapter='trl-lib/llama-7b-hh-rm-adapter', log_with=None, use_safetensors=False, seed=0, use_score_scaling=False, use_score_norm=False, score_clip=None)
2024-08-07 03:03:23,911 - INFO - Loading tokenizer
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-08-07 03:03:24,076 - INFO - Loading dataset: Anthropic/hh-rlhf
2024-08-07 03:03:25,112 - INFO - Dataset loaded. Size: 1608
2024-08-07 03:03:25,113 - INFO - Preparing dataset
2024-08-07 03:03:25,153 - INFO - Dataset preparation completed
2024-08-07 03:03:25,153 - INFO - Configuring LoRA and quantization
2024-08-07 03:03:25,652 - INFO - Using device: cuda
2024-08-07 03:03:25,652 - INFO - Loading model: huggyllama/llama-7b
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.61s/it]
2024-08-07 03:03:29,276 - INFO - Loading dataset: Anthropic/hh-rlhf
2024-08-07 03:03:30,067 - INFO - Dataset loaded. Size: 1608
2024-08-07 03:03:30,068 - INFO - Preparing dataset
2024-08-07 03:03:30,118 - INFO - Dataset preparation completed
2024-08-07 03:03:30,122 - INFO - Model loaded on device: cuda
2024-08-07 03:03:30,122 - INFO - Configuring PPO
2024-08-07 03:03:30,122 - INFO - Initializing PPO Trainer
2024-08-07 03:03:30,472 - INFO - Starting training loop
  0%|                                                                                                                | 0/201 [00:00<?, ?it/s]2024-08-07 03:03:30,477 - INFO - Processing batch in epoch 0
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
  0%|▌                                                                                                       | 1/201 [00:07<24:44,  7.42s/it]2024-08-07 03:03:37,899 - INFO - Processing batch in epoch 1
  0%|▌                                                                                                       | 1/201 [00:07<24:48,  7.44s/it]
Traceback (most recent call last):
  File "/root/collaborative-stegosystem/simple_test/3test.py", line 181, in <module>
    message = generate_alice_message(model, tokenizer, q.unsqueeze(0), d, generation_kwargs)
  File "/root/collaborative-stegosystem/simple_test/3test.py", line 113, in generate_alice_message
    outputs = model.generate(
  File "/usr/local/lib/python3.10/dist-packages/trl/models/modeling_value_head.py", line 209, in generate
    return self.pretrained_model.generate(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 2024, in generate
    result = self._sample(
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 3020, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0