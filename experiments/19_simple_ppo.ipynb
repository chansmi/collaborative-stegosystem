{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set TOKENIZERS_PARALLELISM to true\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Load models and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'  # Set padding side to left for decoder-only models\n",
    "\n",
    "# Create sender and receiver models\n",
    "sender_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "receiver_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token ID for models\n",
    "sender_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "receiver_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# PPO Configuration\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=64,\n",
    "    mini_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    ppo_epochs=5,\n",
    "    max_grad_norm=0.5,\n",
    ")\n",
    "\n",
    "# Define colors and payloads\n",
    "COLORS = [\"red\", \"green\", \"blue\", \"yellow\"]\n",
    "PAYLOADS = [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset\n",
    "def create_dataset(num_samples=1000):\n",
    "    logger.info(f\"Creating dataset with {num_samples} samples...\")\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        payload = random.choice(PAYLOADS)\n",
    "        data.append({\n",
    "            \"payload\": payload,\n",
    "            \"query\": f\"Payload: {payload}, Choose a color:\"\n",
    "        })\n",
    "    logger.info(f\"Dataset created with {len(data)} samples\")\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "dataset = create_dataset()\n",
    "\n",
    "# Create PPO Trainers\n",
    "logger.info(\"Creating PPO Trainers...\")\n",
    "sender_trainer = PPOTrainer(ppo_config, sender_model, tokenizer=tokenizer, dataset=dataset)\n",
    "receiver_trainer = PPOTrainer(ppo_config, receiver_model, tokenizer=tokenizer, dataset=dataset)\n",
    "logger.info(\"PPO Trainers created\")\n",
    "\n",
    "# Define generation kwargs\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epoch\"):\n",
    "    logger.info(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch_idx, batch in enumerate(tqdm(sender_trainer.dataloader, desc=\"Batch\")):\n",
    "        logger.info(f\"Processing batch {batch_idx+1}\")\n",
    "        # Sender step\n",
    "        sender_responses = []\n",
    "        for query in batch[\"query\"]:\n",
    "            logger.info(f\"Sender processing query: {query}\")\n",
    "            query_tensor = tokenizer.encode(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            attention_mask = torch.ones_like(query_tensor)\n",
    "            query_tensor = query_tensor.to(sender_model.pretrained_model.device)\n",
    "            attention_mask = attention_mask.to(sender_model.pretrained_model.device)\n",
    "            \n",
    "            logger.info(f\"Query tensor shape: {query_tensor.shape}\")\n",
    "            logger.info(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "            \n",
    "            response = sender_trainer.generate(query_tensor.squeeze(0), attention_mask=attention_mask.squeeze(0), **generation_kwargs)\n",
    "            decoded_response = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "            logger.info(f\"Sender response: {decoded_response}\")\n",
    "            sender_responses.append(decoded_response)\n",
    "        \n",
    "        # Receiver step\n",
    "        receiver_responses = []\n",
    "        for sender_response, payload in zip(sender_responses, batch[\"payload\"]):\n",
    "            receiver_query = f\"Color: {sender_response}, Guess the payload:\"\n",
    "            logger.info(f\"Receiver processing query: {receiver_query}\")\n",
    "            receiver_query_tensor = tokenizer.encode(receiver_query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            attention_mask = torch.ones_like(receiver_query_tensor)\n",
    "            receiver_query_tensor = receiver_query_tensor.to(receiver_model.pretrained_model.device)\n",
    "            attention_mask = attention_mask.to(receiver_model.pretrained_model.device)\n",
    "            \n",
    "            response = receiver_trainer.generate(receiver_query_tensor.squeeze(0), attention_mask=attention_mask.squeeze(0), **generation_kwargs)\n",
    "            decoded_response = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "            logger.info(f\"Receiver response: {decoded_response}\")\n",
    "            receiver_responses.append(decoded_response)\n",
    "        \n",
    "        # Calculate rewards\n",
    "        rewards = []\n",
    "        for original_payload, receiver_response in zip(batch['payload'], receiver_responses):\n",
    "            try:\n",
    "                guessed_payload = int(receiver_response.strip())\n",
    "                reward = 1.0 if guessed_payload == original_payload else -1.0\n",
    "            except ValueError:\n",
    "                reward = -1.0  # Invalid response\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        rewards_tensor = torch.tensor(rewards).to(sender_model.pretrained_model.device)\n",
    "        \n",
    "        # Update sender\n",
    "        logger.info(\"Updating sender model\")\n",
    "        sender_stats = sender_trainer.step(\n",
    "            [tokenizer.encode(q, return_tensors=\"pt\").squeeze(0).to(sender_model.pretrained_model.device) for q in batch[\"query\"]],\n",
    "            [tokenizer.encode(r, return_tensors=\"pt\").squeeze(0).to(sender_model.pretrained_model.device) for r in sender_responses],\n",
    "            rewards_tensor\n",
    "        )\n",
    "        \n",
    "        # Update receiver\n",
    "        logger.info(\"Updating receiver model\")\n",
    "        receiver_stats = receiver_trainer.step(\n",
    "            [tokenizer.encode(f\"Color: {r}, Guess the payload:\", return_tensors=\"pt\").squeeze(0).to(receiver_model.pretrained_model.device) for r in sender_responses],\n",
    "            [tokenizer.encode(r, return_tensors=\"pt\").squeeze(0).to(receiver_model.pretrained_model.device) for r in receiver_responses],\n",
    "            rewards_tensor\n",
    "        )\n",
    "        \n",
    "        # Log statistics\n",
    "        logger.info(f\"Epoch {epoch+1}, Batch {batch_idx+1}:\")\n",
    "        logger.info(f\"  Sender loss: {sender_stats['ppo/loss/total']:.4f}\")\n",
    "        logger.info(f\"  Receiver loss: {receiver_stats['ppo/loss/total']:.4f}\")\n",
    "        logger.info(f\"  Mean reward: {rewards_tensor.mean().item():.4f}\")\n",
    "\n",
    "# Save models\n",
    "logger.info(\"Saving models...\")\n",
    "sender_trainer.save_pretrained(\"sender_ppo_model\")\n",
    "receiver_trainer.save_pretrained(\"receiver_ppo_model\")\n",
    "\n",
    "logger.info(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation\n",
    "def evaluate(num_samples=100):\n",
    "    logger.info(f\"Starting evaluation with {num_samples} samples...\")\n",
    "    correct = 0\n",
    "    for i in range(num_samples):\n",
    "        payload = random.choice(PAYLOADS)\n",
    "        sender_input = tokenizer.encode(f\"Payload: {payload}, Choose a color:\", return_tensors=\"pt\").to(sender_model.pretrained_model.device)\n",
    "        attention_mask = torch.ones_like(sender_input).to(sender_model.pretrained_model.device)\n",
    "        color_response = sender_trainer.generate(sender_input.squeeze(0), attention_mask=attention_mask.squeeze(0), **generation_kwargs)\n",
    "        color = tokenizer.decode(color_response[0], skip_special_tokens=True)\n",
    "        \n",
    "        receiver_input = tokenizer.encode(f\"Color: {color}, Guess the payload:\", return_tensors=\"pt\").to(receiver_model.pretrained_model.device)\n",
    "        attention_mask = torch.ones_like(receiver_input).to(receiver_model.pretrained_model.device)\n",
    "        guess_response = receiver_trainer.generate(receiver_input.squeeze(0), attention_mask=attention_mask.squeeze(0), **generation_kwargs)\n",
    "        guess = tokenizer.decode(guess_response[0], skip_special_tokens=True)\n",
    "        \n",
    "        logger.info(f\"Evaluation sample {i+1}: Payload={payload}, Color={color}, Guess={guess}\")\n",
    "        \n",
    "        try:\n",
    "            if int(guess) == payload:\n",
    "                correct += 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    accuracy = correct / num_samples\n",
    "    logger.info(f\"Evaluation completed. Accuracy: {accuracy:.2f}\")\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate()\n",
    "logger.info(f\"Final accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
